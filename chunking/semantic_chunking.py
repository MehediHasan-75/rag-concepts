from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import re


def perform_semantic_chunking(document, chunk_size=500, chunk_overlap=100):
    """
    Performs semantic-aware chunking using a recursive, hierarchical splitting strategy.

    The text is divided using progressively finer separators (paragraph → line → sentence → word → character).
    If a chunk exceeds the specified chunk_size, the splitter recursively applies the next
    separator until all chunks satisfy the size constraint.

    Args:
        document (str): Raw input text to be chunked.
        chunk_size (int): Target maximum size (in characters) per chunk.
        chunk_overlap (int): Number of overlapping characters between consecutive chunks
                             to preserve contextual continuity.

    Returns:
        List[Document]: A list of LangChain Document objects enriched with semantic metadata.
    """

    # Initialize recursive splitter with ordered semantic boundaries.
    # The splitter attempts larger logical boundaries first before falling back
    # to smaller units to preserve semantic coherence.
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ". ", " ", ""],  # Hierarchical separator strategy
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len  # Character-based length calculation
    )

    # Perform recursive semantic splitting
    semantic_chunks = text_splitter.split_text(document)
    print(f"Document split into {len(semantic_chunks)} semantic chunks")

    # Define regex patterns to detect structural section headers
    # This enables contextual metadata tagging for improved retrieval filtering.
    section_patterns = [
        r'^#+\s+(.+)$',      # Markdown-style headers (e.g., ## Section)
        r'^.+\n[=\-]{2,}$',  # Underlined headers
        r'^[A-Z\s]+:$'       # ALL CAPS titles ending with colon
    ]

    documents = []
    current_section = "Introduction"  # Default section fallback

    for i, chunk in enumerate(semantic_chunks):

        # Attempt to infer section title from chunk content
        # Taking the last matched headers (pattern)
        chunk_lines = chunk.split('\n')
        for line in chunk_lines:
            for pattern in section_patterns:
                match = re.match(pattern, line, re.MULTILINE)
                if match:
                    current_section = match.group(0)
                    break

        # Compute a lightweight semantic density metric
        # Measures proportion of meaningful (non-stopword) words
        # Useful for downstream ranking or filtering heuristics
        words = re.findall(r'\b\w+\b', chunk.lower())
        stopwords = [
            'the', 'and', 'is', 'of', 'to', 'a', 'in',
            'that', 'it', 'with', 'as', 'for'
        ]
        content_words = [w for w in words if w not in stopwords]
        semantic_density = len(content_words) / max(1, len(words))

        # Wrap chunk into a LangChain Document object
        # Metadata enables advanced filtering, analytics, and traceability
        doc = Document(
            page_content=chunk,
            metadata={
                "chunk_id": i,
                "total_chunks": len(semantic_chunks),
                "chunk_size": len(chunk),
                "chunk_type": "semantic",
                "section": current_section,
                "semantic_density": round(semantic_density, 2)
            }
        )

        documents.append(doc)

    return documents


# ---------------------------------------------------------------------------
# Example Usage
# ---------------------------------------------------------------------------
if __name__ == "__main__":

    # Load source document generated by the test data generator
    try:
        with open("docs/chunking/fixed_size_chunk/rag_chunking_test_doc.md", "r", encoding="utf-8") as f:
            document = f.read()
    except FileNotFoundError:
        raise FileNotFoundError(
            "Test document not found. Run chunking/md_generator.py to create docs/chunking/fixedSizeChunk/rag_chunking_test_doc.md"
        )

    # Execute semantic chunking pipeline
    chunked_docs = perform_semantic_chunking(
        document,
        chunk_size=500,
        chunk_overlap=100
    )

    print("\n----- CHUNKING RESULTS -----")
    print(f"Total semantic chunks: {len(chunked_docs)}")

    # Display representative middle chunk for inspection
    print("\n----- EXAMPLE SEMANTIC CHUNK -----")
    middle_chunk_idx = len(chunked_docs) // 2
    example_chunk = chunked_docs[middle_chunk_idx]

    print(f"Chunk {middle_chunk_idx} content ({len(example_chunk.page_content)} characters):")
    print("-" * 40)
    print(example_chunk.page_content)
    print("-" * 40)
    print(f"Metadata: {example_chunk.metadata}")

    # Analyze chunk distribution across detected sections
    section_counts = {}
    for doc in chunked_docs:
        section = doc.metadata["section"]
        section_counts[section] = section_counts.get(section, 0) + 1

    print("\n----- SECTION DISTRIBUTION -----")
    for section, count in section_counts.items():
        print(f"{section}: {count} chunks")

    # -----------------------------------------------------------------------
    # Databricks Integration Guidance
    # -----------------------------------------------------------------------
    print("\nTo integrate with Databricks:")
    print("1. Generate embeddings using Databricks Embedding API.")
    print("2. Persist Document objects and embeddings in a Delta table.")
    print("3. Create a Vector Search index leveraging semantic metadata for filtering.")
